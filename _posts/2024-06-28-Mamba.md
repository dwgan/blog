---
layout: post
title: "Mamba"
date: 2024-06-28
---

## Introduction

Recently, the most popular language models like OpenAI's ChatGPT, Google's Gemini, and GitHub's Copilot are all based on Transformer architecture. However, Transformers have a significant drawback due to their use of attention mechanisms, which result in quadratic growth in computational demands as sequence length increases. Simply put, for quick interactions (such as asking ChatGPT to tell a joke), this is manageable. However, for tasks requiring extensive word processing (such as summarizing a 100-page document), Transformers can become prohibitively slow. Mamba aims to address this issue by offering better performance than similarly sized Transformers, with computational requirements scaling linearly with sequence length.

## Some tutorial

[CUDA学习第18课：Blelloch扫描算法以及如何选择扫描算法_哔哩哔哩_bilibili](https://www.bilibili.com/video/av94750027/?vd_source=9527f83dc4c2583f0a86df8c039ce605)

[State Space Duality (Mamba-2) Part I - The Model](https://tridao.me/blog/2024/mamba2-part1-model/)

[A Visual Guide to Mamba and State Space Models - Maarten Grootendorst](https://www.maartengrootendorst.com/blog/mamba/)

## Some Papers

### State Space Model (SSM)

 (NeurIPS 2020 Spotlight) HiPPO: Recurrent Memory with Optimal Polynomial Projections [Paper](https://arxiv.org/abs/2008.07669) [Code](https://github.com/HazyResearch/hippo-code) ![Stars](https://img.shields.io/github/stars/HazyResearch/hippo-code)

 (ICLR 2022) S4: Efficiently Modeling Long Sequences with Structured State Spaces [Paper](https://arxiv.org/abs/2111.00396v3) [Code ](https://github.com/state-spaces/s4)![Stars](https://img.shields.io/github/stars/state-spaces/s4)

 (ICLR 2023) H3: Hungry Hungry Hippos: Toward Language Modeling with State Space Models [Paper](https://arxiv.org/abs/2212.14052) [Code](https://github.com/HazyResearch/H3) ![Stars](https://img.shields.io/github/stars/HazyResearch/H3)

(Arxiv 24.05.26) A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models [Paper](https://arxiv.org/abs/2405.16504) [Code](https://github.com/Itamarzimm/UnifiedImplicitAttnRepr) ![Stars](https://img.shields.io/github/stars/Itamarzimm/UnifiedImplicitAttnRepr)

(Arxiv 24.05.27) The Expressive Capacity of State Space Models: A Formal Language Perspective [Paper](https://arxiv.org/abs/2405.17394) [Code](https://github.com/LeapLabTHU/MLLA) ![Stars](https://img.shields.io/github/stars/LeapLabTHU/MLLA)

**(Arxiv 24.06.12) An Empirical Study of Mamba-based Language Models** [Paper](https://arxiv.org/abs/2406.07887)

### Mamba

**(Arxiv 23.12.01) Mamba: Linear-Time Sequence Modeling with Selective State Spaces** [Paper](https://arxiv.org/abs/2312.00752) [Code](https://github.com/state-spaces/mamba) ![Stars](https://img.shields.io/github/stars/state-spaces/mamba)

**(Arxiv 24.05.31, ICML24, Mamba-2) Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality** [Paper](https://arxiv.org/abs/2405.21060)  [Code](https://github.com/state-spaces/mamba) ![Stars](https://img.shields.io/github/stars/state-spaces/mamba)

### Vision

**(Arxiv 24.01.17) Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model** [Paper](https://arxiv.org/abs/2401.09417) [Code](https://github.com/hustvl/Vim) ![Stars](https://img.shields.io/github/stars/hustvl/Vim)

**(Arxiv 24.01.18) VMamba: Visual State Space Model** [Paper](https://arxiv.org/abs/2401.10166) [Code](https://github.com/MzeroMiko/VMamba) ![Stars](https://img.shields.io/github/stars/MzeroMiko/VMamba)

(Arxiv 24.02.05) Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining [Paper](https://arxiv.org/abs/2402.03302) [Code](https://github.com/JiarunLiu/Swin-UMamba) ![Stars](https://img.shields.io/github/stars/JiarunLiu/Swin-UMamba)

(Arxiv 24.02.06) U-shaped Vision Mamba for Single Image Dehazing [Paper](https://arxiv.org/abs/2402.04139) [Code](https://github.com/zzr-idam/UVM-Net) ![Stars](https://img.shields.io/github/stars/zzr-idam/UVM-Net)

(Arxiv 24.02.23) MambaIR: A Simple Baseline for Image Restoration with State-Space Model [Paper](https://arxiv.org/abs/2402.15648) [Code](https://github.com/csguoh/MambaIR) ![Stars](https://img.shields.io/github/stars/csguoh/MambaIR)

(Arxiv 24.03.15) EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba [Paper](https://arxiv.org/abs/2403.09977) [Code](https://github.com/TerryPei/EfficientVMamba) ![Stars](https://img.shields.io/github/stars/TerryPei/EfficientVMamba)

(Arxiv 24.03.28) RSMamba: Remote Sensing Image Classification with State Space Model [Paper](https://arxiv.org/abs/2403.19654) [Code](https://github.com/KyanChen/RSMamba) ![Stars](https://img.shields.io/github/stars/KyanChen/RSMamba)

**(Arxiv 24.05.26) Demystify Mamba in Vision: A Linear Attention Perspective** [Paper](https://arxiv.org/abs/2405.16605) [Code](https://github.com/LeapLabTHU/MLLA) ![Stars](https://img.shields.io/github/stars/LeapLabTHU/MLLA)

**(Arxiv 24.06.04) GrootVL: Tree Topology is All You Need in State Space Model** [Paper](https://arxiv.org/abs/2406.02395) [Code](https://github.com/EasonXiao-888/GrootVL) ![Stars](https://img.shields.io/github/stars/EasonXiao-888/GrootVL)
